{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extract_Brands_Trends.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmcHsH8wxmP3",
        "outputId": "e6e7b252-40b5-4f1f-a405-3110c006b039"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUnqBVMtxyKV",
        "outputId": "af1038f0-64b0-4dac-9245-f85cf8907d67"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "from operator import itemgetter "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSF7qIzn4aHr"
      },
      "source": [
        "# path to read the htmls\n",
        "path = \"/content/drive/My Drive/Wissee AI Projects/1. People/Zhengtong Pan/Lyst_helper/lyst_helper/data/\"\n",
        "# identify path to save the result\n",
        "path_result = \"/content/drive/My Drive/Wissee AI Projects/1. People/Zhengtong Pan/Lyst_helper/lyst_helper/results/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L12n94Y9yASR"
      },
      "source": [
        "### Parse Lyst Page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOp8UiQqyDwM"
      },
      "source": [
        "\"\"\"\n",
        "parse lyst page\n",
        "\"\"\"\n",
        "from bs4 import BeautifulSoup\n",
        "def clean_html(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    # soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
        "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
        "        script.extract()\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "    return text\n",
        "\n",
        "# get the text of each page \n",
        "def get_page_text(path, file_name): \n",
        "    _in_html = path + file_name\n",
        "    # print(_in_html)\n",
        "    with open(_in_html) as _out:\n",
        "        html = _out.read()\n",
        "    text =  clean_html(html)\n",
        "    return text\n",
        "\n",
        "def get_text_from_all_pages(html_strings):\n",
        "    # create a list of htmls\n",
        "    # html_strings = \"the-lyst-index-q119.html  the-lyst-index-q220.html  the-lyst-index-q320.html the-lyst-index-q120.html  the-lyst-index-q221.html  the-lyst-index-q418.html the-lyst-index-q121.html  the-lyst-index-q318.html  the-lyst-index-q419.html the-lyst-index-q219.html  the-lyst-index-q319.html  the-lyst-index-q420.html\"\n",
        "    html_lst_ = html_strings.split(\" \")\n",
        "    html_lst = [html for html in html_lst_ if html != \"\"]\n",
        "\n",
        "    # get text from all pages\n",
        "    all_pages_text = []\n",
        "    for html in html_lst: \n",
        "        page_text = get_page_text(path, html)\n",
        "        all_pages_text.append(page_text)\n",
        "    return all_pages_text"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt85IOuLySac"
      },
      "source": [
        "###Extract Notes\n",
        "- split text into list of lists\n",
        "- regex to find the sublist that contains percentage\n",
        "- if  the sentence contaisn \"%\" and starts with verb, combine the sentence with its previous sentence, and the sentence after it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWLATlQ60Flt"
      },
      "source": [
        "def get_string_with_percenatge(text):\n",
        "    text_lst = text.split(\"\\n\")\n",
        "    # print(text_lst)\n",
        "    percenatge_string_lst = []\n",
        "    indexes = []\n",
        "    for i in range(len(text_lst)): \n",
        "        string = text_lst[i]\n",
        "        str_tok = word_tokenize(string)\n",
        "        pos_tag = nltk.pos_tag(str_tok)\n",
        "        # print(str_tok)\n",
        "        # print(nltk.pos_tag(str_tok))\n",
        "        # check whether the string contains number with percenatage or not \n",
        "        m = re.match(r'\\d+%|([0-9]\\d?)\\.\\d+%', string)\n",
        "        # print(m.string)\n",
        "        if \"%\" in string:\n",
        "          # percenatge_string_lst.append(m.string)\n",
        "          # string with percentage but starts with verb or start with number with percenatge only \n",
        "          if (pos_tag[0][1] == \"VBD\" or m ) and i!=0 and i!=len(text_lst)-1:\n",
        "              tmp_before = text_lst[i-1]\n",
        "              tmp_after = text_lst[i+1]\n",
        "              res = tmp_before + \" \" + string\n",
        "              res = res + \" \" + tmp_after\n",
        "              percenatge_string_lst.append(res)\n",
        "              indexes.append(i)\n",
        "          elif re.match(r'NN', pos_tag[0][1]) or re.match(r\"JJ\", pos_tag[0][1]): # sentence with percentage starts with noun or descriptive noun\n",
        "              percenatge_string_lst.append(string)\n",
        "              indexes.append(i)\n",
        "    return percenatge_string_lst, indexes\n",
        "\n",
        "# input: all pages text (list of lists; sublist is a html page)\n",
        "# output: all strings with percentage (list of lists; sublist is the filtered string from each html page)\n",
        "def get_all_strings_with_percentage(all_pages_text):\n",
        "    all_strings_with_percentage = []\n",
        "    all_indexes = []\n",
        "    for page_text in all_pages_text: \n",
        "        percenatge_string_lst, indexes= get_string_with_percenatge(page_text)\n",
        "        all_strings_with_percentage.append(percenatge_string_lst)\n",
        "        all_indexes.append(indexes)\n",
        "    return all_strings_with_percentage, all_indexes"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmuFWNiy0j44"
      },
      "source": [
        "### Extract Quarter Year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb7E18DS0gzC"
      },
      "source": [
        "# input: page text\n",
        "# output: all the quarter year information, sentences with percentage\n",
        "def get_string_with_quarter_year(text):\n",
        "    text_lst = text.split(\"\\n\")\n",
        "    # print(text_lst)\n",
        "    quarter_year_lst = []\n",
        "    percentage_string_lst =[]\n",
        "    quarter_year_indexes = []\n",
        "    percentage_indexes = []\n",
        "    for i in range(len(text_lst)): \n",
        "        string = text_lst[i]\n",
        "        if re.match(r\"Q\", string):\n",
        "            quarter_year_lst.append(string)\n",
        "            quarter_year_indexes.append(i)\n",
        "        if \"%\" in string:\n",
        "            percentage_string_lst.append(string)\n",
        "            percentage_indexes.append(i)\n",
        "    return quarter_year_lst, percentage_string_lst, quarter_year_indexes, percentage_indexes\n",
        "\n",
        "def get_all_strings_with_quarter_year(all_pages_text):\n",
        "    all_strings_quarter_year = []\n",
        "    all_strings_with_percenatges = []\n",
        "    all_quarter_year_ind = []\n",
        "    all_string_with_percentage_ind = []\n",
        "    for page_text in all_pages_text:\n",
        "        quarter_year_lst, percentage_string_lst, quarter_year_indexes, percentage_indexes = get_string_with_quarter_year(page_text)\n",
        "        all_strings_quarter_year.append(quarter_year_lst)\n",
        "        all_strings_with_percenatges.append(percentage_string_lst)\n",
        "        all_quarter_year_ind.append(quarter_year_indexes)\n",
        "        all_string_with_percentage_ind.append(percentage_indexes)\n",
        "    return all_strings_quarter_year, all_strings_with_percenatges, all_quarter_year_ind, all_string_with_percentage_ind"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaKREYrX0w-x"
      },
      "source": [
        "### Extract Brands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eG06ZR_0zru"
      },
      "source": [
        "def identify_entity_text(text, ent_label): \n",
        "    doc = sp(text)\n",
        "    entity_text = []\n",
        "    if doc.ents:\n",
        "       for ent in doc.ents:\n",
        "           if ent.label_ == ent_label:\n",
        "              entity_text.append(ent.text)\n",
        "          # if ent.label_ == \"DATE\":\n",
        "              # years.append(ent.text)\n",
        "    if (entity_text  == []) and ent_label == \"ORG\":\n",
        "        text_tok = word_tokenize(text)\n",
        "        pos_tags = nltk.pos_tag(text_tok)\n",
        "        # print(pos_tags)\n",
        "        for i in range(len(pos_tags)):\n",
        "            # identify. the text that contains number with percenatge\n",
        "            if i!= len(pos_tags)-1 and (pos_tags[i][1] == \"CD\" and pos_tags[i+1][0] == \"%\"):  \n",
        "              candidates = pos_tags[:i] # candidates to find product/brand names\n",
        "              # find indexes of noun\n",
        "              product_brand_indexes = [i for i in range(len(candidates)) if candidates[i][1] in(\"NN\", \"NNP\")]\n",
        "              # print(product_brand_indexes)\n",
        "              # get product/brands\n",
        "              product_brand_tags = itemgetter(*product_brand_indexes)(candidates)\n",
        "              if type(product_brand_tags) == tuple and len(product_brand_tags) == 2: # corner case for a single tuple\n",
        "                  # print(product_brand_tags[0])\n",
        "                  product_brands = [product_brand_tags[0]]\n",
        "                  # print(product_brands)\n",
        "              else:\n",
        "                  product_brands = [tag[0] for tag in product_brand_tags]\n",
        "                  entity_text.append(product_brands)\n",
        "                  # print(entity_text)\n",
        "                  break\n",
        "    elif entity_text == []:\n",
        "          entity_text.append(None)\n",
        "    return entity_text\n",
        "\n",
        "def get_all_entity_text(notes, ent_label, all_notes_indexes):\n",
        "    all_brands = []\n",
        "    all_brands_ind = []\n",
        "    for i in range(len(notes)): # for each page \n",
        "        page_brands = []\n",
        "        page_brand_ind = []\n",
        "        for s in range(len(notes[i])): # for each sentence in the page \n",
        "            brands= identify_entity_text(notes[i][s], ent_label)\n",
        "            page_brands.append(brands)\n",
        "            if brands != [None]: \n",
        "               page_brand_ind.append(all_notes_indexes[i][s])\n",
        "        all_brands.append(page_brands)\n",
        "        all_brands_ind.append(page_brand_ind)\n",
        "    return all_brands, all_brands_ind"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSnQBqUY1Axr"
      },
      "source": [
        "### Data Cleaning\n",
        "\n",
        "#### Year Cleaning:\n",
        "- Match the year with notes: \n",
        "- Get the indexes of the year\n",
        "- Get the indexes of the strings with percentage \n",
        "- Assign the year with the index closest to the index of the string with percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXFm972q1kql"
      },
      "source": [
        "##### Algorithm to find the closest year: \n",
        "\n",
        "for each string_index in Percentage_String_Index:\n",
        " - locate the index within the Year_Index\n",
        " - find the largest year_index which is smaller than the string_Index\n",
        " - page of year for the string_index is the largest year_index smaller than the string_Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUAybbY41Nfh"
      },
      "source": [
        "# binary search to find the largest element that is smller or equal to the current index\n",
        "def find_largest_smaller(array, target):\n",
        "    if not array or len(array) == 0: \n",
        "       return array[-1]\n",
        "    left = 0\n",
        "    right = len(array)-1\n",
        "    while left < right -1: \n",
        "        mid = (left + right)//2\n",
        "        # case1: mid point value < target \n",
        "        if array[mid] < target: \n",
        "           left = mid\n",
        "        # case2: mid point value > target\n",
        "        elif array[mid] > target: \n",
        "            right = mid-1\n",
        "        # case3: mid point value == target\n",
        "        else: \n",
        "            left = mid\n",
        "    # post-processing\n",
        "    # see wehther right or left <= target or neither of them <= target \n",
        "    if array[right] <= target: \n",
        "       return array[right]\n",
        "    if array[left] <= target:\n",
        "       return array[left]\n",
        "    else: \n",
        "       return array[-1]\n",
        "\n",
        "# create percentage string index and percentage string dictionary \n",
        "# year index and year/quarter value dictionary \n",
        "def create_dictionary(value_lst, index_lst):\n",
        "    dict_from_list = dict(zip(index_lst, value_lst))\n",
        "    return dict_from_list\n",
        "\n",
        "def create_feature_dictionary(df_tmp, i):\n",
        "    notes_dict = create_dictionary(df_tmp['Notes'][i], df_tmp['Notes_indexes'][i])\n",
        "    year_dict = create_dictionary(df_tmp['Quarter/Year'][i], df_tmp['Year_Index'][i])\n",
        "    brand_dict = create_dictionary(df_tmp['Product/brand'][i], df_tmp['Product/brand_indexes'][i])\n",
        "\n",
        "    # add percentage string into first_page_notes_dict if it doesn't have the percenatage string \n",
        "    extra_notes_indexes = list(set(df_tmp['Percentage_String_Index'][i]) - set(df_tmp['Notes_indexes'][i]))\n",
        "    extra_notes = list(set(df_tmp['Percentage_strings_only'][i]) - set(df_tmp['Notes'][i]))\n",
        "    extra_notes_dict = create_dictionary(extra_notes, extra_notes_indexes)\n",
        "    notes_dict.update(extra_notes_dict)\n",
        "    return notes_dict, year_dict, brand_dict\n",
        "\n",
        "# get the year index (largest number smaller than current percentage string index) for each percentage string \n",
        "# get the year value based on year index for each percentage string \n",
        "def create_unwrap_dataframe(df_tmp, notes_dict, year_dict, brand_dict, page_ind):\n",
        "    df_example = pd.DataFrame(columns = ['Quarter/Year', \"Product/brand\", 'Notes'])\n",
        "    i = 0\n",
        "    for str_ind in notes_dict.keys(): \n",
        "        year_ind = find_largest_smaller(df_tmp['Year_Index'][page_ind], str_ind)\n",
        "        quarter_year = year_dict[year_ind]\n",
        "        percentage_string = notes_dict[str_ind]\n",
        "        if str_ind in brand_dict:\n",
        "            brand = brand_dict[str_ind]\n",
        "        else: \n",
        "            brand = None\n",
        "        df_example.loc[i] = [quarter_year] + [brand] + [percentage_string]\n",
        "        i += 1\n",
        "    return df_example\n",
        "\n",
        "def create_all_dataframes(df_raw):\n",
        "    frames = None\n",
        "    for page_ind in range(df_raw.shape[0]): \n",
        "        notes_dict, year_dict, brand_dict = create_feature_dictionary(df_raw, page_ind)\n",
        "        df_curr = create_unwrap_dataframe(df_raw, notes_dict, year_dict, brand_dict, page_ind)\n",
        "        if frames is None: \n",
        "           frames = df_curr\n",
        "        else:\n",
        "           result =  pd.concat([frames, df_curr])\n",
        "           frames = result\n",
        "    return frames"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_JNVTKp2Xch"
      },
      "source": [
        "#### Brands Cleaning\n",
        "- Get the organization entity closest to each percentage string within a setence to assign brands\n",
        "- Get the indexes of extracted brands and extracted percentage strings\n",
        "- Use binary search(find largest smaller) within each sentence to find the closest brand for each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1I4g9J03l24"
      },
      "source": [
        "def get_extra_brands(df_null_brands):\n",
        "    all_extra_brands = []\n",
        "    all_extra_brands_ind = []\n",
        "    all_percent_ind = []\n",
        "    for note in df_null_brands['Notes']: \n",
        "        # create entity for each note\n",
        "        extra_brands = []\n",
        "        extra_brands_ind = []\n",
        "        percent_ind = []\n",
        "        doc = sp(note)\n",
        "        if doc.ents: \n",
        "          for i in range(len(doc.ents)):\n",
        "              # recognize percentage and record its index \n",
        "              if doc.ents[i].label_ == \"PERCENT\":\n",
        "                  percent_ind.append(i)\n",
        "              if doc.ents[i].label_ == \"ORG\":\n",
        "                  extra_brands.append(doc.ents[i].text)\n",
        "                  extra_brands_ind.append(i)\n",
        "          all_extra_brands.append(extra_brands)\n",
        "          all_extra_brands_ind.append(extra_brands_ind)\n",
        "          all_percent_ind.append(percent_ind)\n",
        "        else:\n",
        "          all_extra_brands.append(None)\n",
        "          all_extra_brands_ind.append(None)\n",
        "          all_percent_ind.append(None)\n",
        "    return all_extra_brands, all_extra_brands_ind, all_percent_ind\n",
        "\n",
        "def get_closest_brands(df_extra_brands):\n",
        "    # assumptions: percentage index is not null; brands can be; number of percent indexes < number of brand indexes \n",
        "    clean_extra_brands = []\n",
        "    for i in range(df_extra_brands.shape[0]):\n",
        "        percent_lst = df_extra_brands['All_Percent_Ind'][i]\n",
        "        brand_ind_lst = df_extra_brands['Extra_Brands_Ind'][i]\n",
        "        extra_brands_lst = df_extra_brands['Extra_Brands'][i]\n",
        "        # case1: same amount of percentages and brands \n",
        "        if len(percent_lst) == len(brand_ind_lst):\n",
        "          closest_brands = extra_brands_lst\n",
        "          # print(closest_brands)\n",
        "        # case2: 1 index in the percent_lst and >1 indexes in Extra_Brands_Ind\n",
        "        elif len(percent_lst) ==1 and len(brand_ind_lst)>1:\n",
        "          # print(percent_lst[0])\n",
        "          # print(brand_ind_lst)\n",
        "          # binary searh to find largest smaller elements(index)\n",
        "          closest_brand_ind = find_largest_smaller(brand_ind_lst, percent_lst[0])\n",
        "          # print(closest_brand_ind)\n",
        "          # get actual brand index within brand names list \n",
        "          actual_brand_ind = brand_ind_lst.index(closest_brand_ind)\n",
        "          # print(actual_brand_ind)\n",
        "          # get the closest brand\n",
        "          closest_brands = [extra_brands_lst[actual_brand_ind]] \n",
        "          # print(closest_brands)\n",
        "        # case3: >1 indexes in the percent_st and >1 indexes in EXtra_Brands_Ind\n",
        "        elif len(percent_lst) > 1 and len(brand_ind_lst) > 1: \n",
        "            # for each percent_ind in percent_lst, find the closest brand_ind to it \n",
        "            closest_brands = []\n",
        "            for percent_ind in percent_lst: \n",
        "                # get closest brand index \n",
        "                tmp_ind = find_largest_smaller(brand_ind_lst, percent_ind)\n",
        "                # get actual brand indexes within brand names list \n",
        "                actual_brand_ind = brand_ind_lst.index(tmp_ind)\n",
        "                closest_brand = [extra_brands_lst[actual_brand_ind]]\n",
        "                closest_brands.append(closest_brand)\n",
        "            # print(closest_brands)\n",
        "        # case4: can't find brand within the percenatge sentence\n",
        "        else: \n",
        "            closest_brands = [None]\n",
        "        # print(closest_brands)\n",
        "        clean_extra_brands.append(closest_brands)\n",
        "    return clean_extra_brands\n",
        "\n",
        "def unpack_list(lst):\n",
        "    return [item for sublist in lst for item in sublist]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ixo0bg9EG4"
      },
      "source": [
        "### Execute all steps in a flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAaRUzcK9JmH"
      },
      "source": [
        "def main():\n",
        "    # get text from all pages\n",
        "    html_strings = input(\"Enter htmls formatted with qoutes and seperated by a single white space:\")\n",
        "    all_pages_text = get_text_from_all_pages(html_strings)\n",
        "    # extract notes\n",
        "    # get all strings with percentages \n",
        "    all_strings_with_percentage, all_notes_indexes = get_all_strings_with_percentage(all_pages_text)\n",
        "    notes = pd.Series(all_strings_with_percentage)\n",
        "    # extract quarter year\n",
        "    all_quarter_years, all_strings_percentages_only, all_quarter_year_ind, all_string_with_percentage_ind= get_all_strings_with_quarter_year(all_pages_text)\n",
        "    # extract brands\n",
        "    all_brands, all_brands_ind = get_all_entity_text(notes, \"ORG\", all_notes_indexes)\n",
        "\n",
        "    # data cleaning \n",
        "\n",
        "    # year cleaning\n",
        "    # non-cleaned dataframe\n",
        "    # dataframe with year, percentage string indexes \n",
        "    frame = {'Quarter/Year': all_quarter_years, \"Year_Index\": all_quarter_year_ind, \"Product/brand\": all_brands, \"Product/brand_indexes\": all_brands_ind,  \"Notes\": notes, \"Notes_indexes\": all_notes_indexes, \"Percentage_strings_only\": all_strings_percentages_only, \"Percentage_String_Index\": all_string_with_percentage_ind}\n",
        "    df = pd.DataFrame(frame)\n",
        "    df_tmp = df[['Quarter/Year',\t'Year_Index', \"Product/brand\", \"Product/brand_indexes\", 'Percentage_strings_only',\t'Percentage_String_Index', \"Notes\", \"Notes_indexes\"]]\n",
        "    df_all = create_all_dataframes(df_tmp)\n",
        "    # further cleaning of year data\n",
        "    new_quarter_year = []\n",
        "    for year in df_all['Quarter/Year']:\n",
        "        if \"HOTTEST WOMEN'S PRODUCTS\" in year: \n",
        "            new_year = year.replace(\"HOTTEST WOMEN'S PRODUCTS\", \"\")\n",
        "            new_quarter_year.append(new_year)\n",
        "        else:\n",
        "            new_quarter_year.append(year)\n",
        "    df_all['Quarter/Year'] = new_quarter_year\n",
        "    df_all[['Quarter', \"Year\"]] = df_all['Quarter/Year'].str.split(expand=True)\n",
        "    df_all.drop(['Quarter/Year'], axis=1, inplace=True)\n",
        "    # reorganize the columns\n",
        "    df_all_new = df_all[[\"Year\", \"Quarter\", \"Product/brand\", \"Notes\"]]\n",
        "\n",
        "    #brands cleaning\n",
        "    all_filter_brands, all_filter_brands_ind, all_percent_ind = get_extra_brands(df_all_new)\n",
        "    filter_brands_frame = {\"Extra_Brands\": all_filter_brands, \"Extra_Brands_Ind\": all_filter_brands_ind, \"All_Percent_Ind\": all_percent_ind}\n",
        "    df_filter_brands = pd.DataFrame(filter_brands_frame)\n",
        "\n",
        "    # format all closest brands into one list of brands per sentence\n",
        "    all_closest_brands = get_closest_brands(df_filter_brands)\n",
        "    all_closest_brands_unpack = [ unpack_list(brands)if len(brands)>1 and type(brands[0]) == list else brands for brands in all_closest_brands]\n",
        "    # replace cleaned brand values for all brands  \n",
        "    df_filter_brands = df_all_new.copy()\n",
        "    df_filter_brands['Product/brand'] = all_closest_brands_unpack\n",
        "\n",
        "    # save the result\n",
        "    df_filter_brands.to_excel(path_result + \"lyst_fashion_trend_final.xlsx\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L10PrPtx9MmS",
        "outputId": "19228354-b777-4964-ddbf-e26da5d62d6d"
      },
      "source": [
        "main()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter htmls formatted with qoutes and seperated by a single white space:the-lyst-index-q119.html  the-lyst-index-q220.html  the-lyst-index-q320.html the-lyst-index-q120.html  the-lyst-index-q221.html  the-lyst-index-q418.html the-lyst-index-q121.html  the-lyst-index-q318.html  the-lyst-index-q419.html the-lyst-index-q219.html  the-lyst-index-q319.html  the-lyst-index-q420.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO4H5O5f9OeE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}